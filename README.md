# AgentsVille Trip Planner ğŸ§­ğŸ¤–
**Agentic AI travel assistant** that generates and *refines* day-by-day itineraries for a (fictional) city, **AgentsVille**.  
It combines **structured prompting**, **Pydantic models**, a **ReAct revision agent** with tool use, and **LLM-powered evaluations** (including weather/activity compatibility).

<p align="center">
  <img src="assets/agent.jpg" alt="AgentsVille Trip Planner Banner" width="1000"/>
</p>

<p align="center">
  <img src="assets/img_7974.jpg" alt="Evaluation Screenshot 1" width="600"/>
  <br/>
  <img src="assets/img_7975.jpg " alt="Evaluation Screenshot 2" width="600"/>
</p>

## âœ¨ Highlights
- **Expert Planner (ItineraryAgent):** Produces a structured `TravelPlan` JSON that validates against Pydantic models.
- **ReAct Revision Agent:** Uses THINK â†’ ACT â†’ OBSERVE cycles to fetch activities, run evals, and revise the plan.
- **Tooling:** calculator, get_activities_by_date, run_evals, final_answer (with explicit JSON call format).
- **LLM Evals:** Checks dates/city, cost accuracy, interests coverage, **and weather vs. activities** via a dedicated system prompt.
- **Prompt Engineering:** Role, Task, Output Format, Context, and targeted Examples to elicit reliable structured outputs.

---

## ğŸ§© What this project demonstrates 
- Designing **agentic systems** (multi-agent orchestration & feedback loops)
- **Reasoning-focused prompts** (CoT/ReAct) with deterministic output shapes
- **Structured validation** with **Pydantic** (enums, nested models, dates)
- **LLM tool use** (clear schemas + docstrings â†’ fewer hallucinations, cleaner calls)
- **Eval-driven iteration** (fixing failures programmatically & via prompts)

---

## ğŸ—ºï¸ Project Architecture

```text
.
â”œâ”€â”€ project_starter.ipynb      # Main Jupyter app: prompts, agents, evals, runs
â”œâ”€â”€ project_lib.py             # Pydantic models, mocked tools, utilities
â”œâ”€â”€ assets/                    # Screenshots for README
â”‚   â”œâ”€â”€ screenshot-itinerary-pass.png
â”‚   â””â”€â”€ screenshot-evals.png
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore

Core Components
	â€¢	Pydantic models: VacationInfo, Traveler, Activity, ItineraryDay, TravelPlan
	â€¢	ItineraryAgent: Generates initial plan â†’ JSON conforming to TravelPlan.model_json_schema()
	â€¢	LLM Evals:
	â€¢	start/end dates match
	â€¢	total cost accurate & within budget
	â€¢	interests coverage
	â€¢	weather compatibility (uses ACTIVITY_AND_WEATHER_ARE_COMPATIBLE_SYSTEM_PROMPT)
	â€¢	ReAct Revision Agent:
	â€¢	Runs run_evals_tool â†’ fixes issues (e.g., outdoor activity during storms) â†’ runs evals again â†’ calls final_answer_tool

â¸»

ğŸ§ª Example Prompts (snippets)

Weather & Activity Compatibility (excerpt)
	â€¢	Guideline: If an event explicitly moves indoors (e.g., â€œmoves indoors to Harmony Hall if it rainsâ€), treat as IS_COMPATIBLE even in thunderstorms.
	â€¢	Final Answer: [IS_COMPATIBLE] or [IS_INCOMPATIBLE] (strict)

Itinerary Agent (excerpt)
	â€¢	Role: Itinerary Planning Agent
	â€¢	Task: Analyze weather + activities + interests; produce day-by-day plan; keep running cost; respect budget.
	â€¢	Output: Two sections â€” ANALYSIS and FINAL OUTPUT where FINAL OUTPUT is valid TravelPlan JSON.

â¸»

ğŸš€ Getting Started

Requires Python 3.10+ and Jupyter.

	1.	Clone or download this repo, then open project_starter.ipynb in Jupyter.
	2.	API key: The notebook uses mocked tools; LLM calls require an OpenAI-compatible client.
	â€¢	Set env var: export OPENAI_API_KEY=... (or use the cell provided in the notebook).
	3.	Run the notebook top-to-bottom:
	â€¢	Fill VacationInfo
	â€¢	Generate initial plan (ItineraryAgent)
	â€¢	Run Evals
	â€¢	Iterate with the ReAct Revision Agent until all evals pass âœ…

Note: This repo includes mocked APIs for activities & weather to keep runs deterministic and reviewable.

â¸»

ğŸ“Š Reproducing the passing result
	â€¢	Open project_starter.ipynb
	â€¢	Run cells until evaluations; if any fail, the ReAct agent will:
	â€¢	call get_activities_by_date_tool to fetch catalog
	â€¢	update activities for weather/interest/budget
	â€¢	re-run run_evals_tool
	â€¢	Final cell prints:
All evaluation functions passed successfully for the revised travel plan.

â¸»

ğŸ”§ Tools (docstrings summarized)
	â€¢	calculator_tool: math/cost aggregation
	â€¢	get_activities_by_date_tool(date: str, city: str): returns valid Activity dicts; date="YYYY-MM-DD"; city is case-sensitive
	â€¢	run_evals_tool: runs the full evaluation suite
	â€¢	final_answer_tool: ends the ReAct loop; returns final TravelPlan

â¸»

ğŸ§± Pydantic Models (examples)
	â€¢	VacationInfo: destination, date_of_arrival, date_of_departure, budget, travelers: List[Traveler]
	â€¢	TravelPlan: city, start_date, end_date, total_cost, itinerary_days: List[ItineraryDay]

â¸»

ğŸ“¸ Screens & Demo

Add your screenshots to /assets and reference them here:
	â€¢	Passing evals: assets/screenshot-itinerary-pass.png
	â€¢	ReAct traces or prompt boxes: assets/screenshot-evals.png

â¸»

ğŸ™‹â€â™€ï¸ FAQ

Q: Why mock tools instead of hitting live APIs?
A: Keeps the project deterministic, fast to run, and reviewer-friendly while still demonstrating tool use.

Q: Where are the prompts?
A: In the notebook: ITINERARY_AGENT_SYSTEM_PROMPT, ACTIVITY_AND_WEATHER_ARE_COMPATIBLE_SYSTEM_PROMPT, and the ItineraryRevisionAgent system prompt.

â¸»

ğŸ“ License

MIT

ğŸ’Œ Author

Sherri (SK) â€” Agentic AI enthusiast | AWS learner | Tech fan ğŸ¬ğŸŒŠ
